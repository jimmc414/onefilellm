STDOUT:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                              â”‚
â”‚  TEST RECORDING SYSTEM                                                       â”‚
â”‚  Automated pytest snapshot test generation from actual usage                 â”‚
â”‚                                                                              â”‚
â”‚  ğŸ¯ OVERVIEW                                                                 â”‚
â”‚  The --record-test feature allows you to automatically generate pytest       â”‚
â”‚  snapshot tests                                                              â”‚
â”‚  by recording the actual execution of OneFileLLM commands. This creates      â”‚
â”‚  reproducible                                                                â”‚
â”‚  tests that verify consistent behavior across code changes.                  â”‚
â”‚                                                                              â”‚
â”‚  ğŸš€ BASIC USAGE                                                              â”‚
â”‚    # Record a simple file processing test                                    â”‚
â”‚    python onefilellm.py --record-test simple-file test.txt                   â”‚
â”‚                                                                              â”‚
â”‚    # Record a help command test                                              â”‚
â”‚    python onefilellm.py --record-test help-basic --help-topic basic          â”‚
â”‚                                                                              â”‚
â”‚    # Record a GitHub repository test                                         â”‚
â”‚    python onefilellm.py --record-test github-repo                            â”‚
â”‚  https://github.com/user/repo                                                â”‚
â”‚                                                                              â”‚
â”‚  ğŸ“ GENERATED FILES                                                          â”‚
â”‚  When you use --record-test, two files are created:                          â”‚
â”‚                                                                              â”‚
â”‚  1. Test File: tests/test_recorded_<TEST_NAME>.py                            â”‚
â”‚     - Contains the pytest test function                                      â”‚
â”‚     - Uses the test harness for subprocess execution                         â”‚
â”‚     - Compares output against the snapshot                                   â”‚
â”‚                                                                              â”‚
â”‚  2. Snapshot File: tests/snapshots/<TEST_NAME>_<HASH>.txt                    â”‚
â”‚     - Contains the captured output (stdout + stderr)                         â”‚
â”‚     - Includes exit codes if non-zero                                        â”‚
â”‚     - Used for comparison in future test runs                                â”‚
â”‚                                                                              â”‚
â”‚  ğŸ”§ HOW IT WORKS                                                             â”‚
â”‚  1. Argument Parsing: The --record-test flag is detected early               â”‚
â”‚  2. Command Execution: Your command runs in a subprocess                     â”‚
â”‚  3. Output Capture: All stdout, stderr, and exit codes are captured          â”‚
â”‚  4. Test Generation: A pytest file is created with proper imports            â”‚
â”‚  5. Snapshot Creation: Output is saved for future comparisons                â”‚
â”‚                                                                              â”‚
â”‚  âœ… RUNNING GENERATED TESTS                                                  â”‚
â”‚    # Run a specific recorded test                                            â”‚
â”‚    pytest tests/test_recorded_simple-file.py -v                              â”‚
â”‚                                                                              â”‚
â”‚    # Run all recorded tests                                                  â”‚
â”‚    pytest tests/test_recorded_*.py -v                                        â”‚
â”‚                                                                              â”‚
â”‚    # Run with snapshot update (if output changed intentionally)              â”‚
â”‚    pytest tests/test_recorded_*.py --snapshot-update                         â”‚
â”‚                                                                              â”‚
â”‚  ğŸ’¡ BEST PRACTICES                                                           â”‚
â”‚  â€¢ Use descriptive test names that indicate what's being tested              â”‚
â”‚  â€¢ Record tests for common use cases and edge cases                          â”‚
â”‚  â€¢ Review generated snapshots to ensure they capture expected output         â”‚
â”‚  â€¢ Update snapshots when output changes are intentional                      â”‚
â”‚  â€¢ Include recorded tests in your CI/CD pipeline                             â”‚
â”‚                                                                              â”‚
â”‚  ğŸ¨ EXAMPLE SCENARIOS                                                        â”‚
â”‚    # Test local file processing with specific format                         â”‚
â”‚    python onefilellm.py --record-test json-processing data.json --format     â”‚
â”‚  json                                                                        â”‚
â”‚                                                                              â”‚
â”‚    # Test web crawling with parameters                                       â”‚
â”‚    python onefilellm.py --record-test web-crawl-test https://example.com \   â”‚
â”‚      --crawl-max-depth 2 --crawl-max-pages 10                                â”‚
â”‚                                                                              â”‚
â”‚    # Test alias expansion                                                    â”‚
â”‚    python onefilellm.py --record-test alias-test ofl_repo                    â”‚
â”‚                                                                              â”‚
â”‚    # Test multiple inputs                                                    â”‚
â”‚    python onefilellm.py --record-test multi-input file1.txt file2.py dir/    â”‚
â”‚                                                                              â”‚
â”‚    # Test stream processing                                                  â”‚
â”‚    echo 'test content' | python onefilellm.py --record-test stdin-test -     â”‚
â”‚                                                                              â”‚
â”‚  âš ï¸  LIMITATIONS                                                              â”‚
â”‚  â€¢ Test names must be valid Python identifiers (alphanumeric + underscores)  â”‚
â”‚  â€¢ The --record-test flag itself is not included in the recorded command     â”‚
â”‚  â€¢ Interactive prompts are not supported in recorded tests                   â”‚
â”‚  â€¢ Tests with time-sensitive or random output may need special handling      â”‚
â”‚                                                                              â”‚
â”‚  ğŸ” ADVANCED USAGE                                                           â”‚
â”‚    # Record a complex pipeline test                                          â”‚
â”‚    python onefilellm.py --record-test pipeline-test \                        â”‚
â”‚      https://github.com/microsoft/vscode/issues/1234 \                       â”‚
â”‚      https://github.com/microsoft/vscode/pull/5678 \                         â”‚
â”‚      --crawl-max-depth 1                                                     â”‚
â”‚                                                                              â”‚
â”‚    # Test error handling                                                     â”‚
â”‚    python onefilellm.py --record-test error-test /nonexistent/file.txt       â”‚
â”‚                                                                              â”‚
â”‚  ğŸ“Š TEST ORGANIZATION                                                        â”‚
â”‚  Generated tests follow this structure:                                      â”‚
â”‚                                                                              â”‚
â”‚    tests/                                                                    â”‚
â”‚    â”œâ”€â”€ harness.py              # Subprocess execution framework              â”‚
â”‚    â”œâ”€â”€ test_recorded_*.py      # Generated test files                        â”‚
â”‚    â””â”€â”€ snapshots/              # Snapshot output files                       â”‚
â”‚        â””â”€â”€ <test_name>_<hash>.txt                                            â”‚
â”‚                                                                              â”‚
â”‚  ğŸš§ TROUBLESHOOTING                                                          â”‚
â”‚  If test recording fails:                                                    â”‚
â”‚  â€¢ Ensure all dependencies are installed (pytest, pytest-snapshot)           â”‚
â”‚  â€¢ Check that the tests/ directory exists and is writable                    â”‚
â”‚  â€¢ Verify the test name is valid (no spaces or special characters)           â”‚
â”‚  â€¢ Make sure the command works without --record-test first                   â”‚
â”‚                                                                              â”‚
â”‚                                                                              â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯